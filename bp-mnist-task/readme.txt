Network Structure:
	Input layer: 784(28*28) neurons
	1 hidden layer: 100 nerons per layer
	Output layer: Softmax output (10 neurons)

I tested 2 activation functions for hidden layers:
	1. ReLU: performs good ( accuracy reached 90% in 10 epoches )
	2. Sigmoid: not so effecient ( accuracy reached 90% in 50 epoches )

------------------------

Derivation of backprop for MLP with Sigmoid, see my personal blog: 
	http://www.cosmozhang.com/2016/12/11/mathematics-of-bp-nn.html

------------------------

Name:         Cosmo Zhang
Student No.:  1601110508

